# LLM APIå®¢æˆ·ç«¯
"""
LLMå®¢æˆ·ç«¯æ¨¡å—
=============
ç»Ÿä¸€å°è£…AIæ¨¡å‹è°ƒç”¨ï¼Œå…¨è¯¾ç¨‹å¤ç”¨
æ”¯æŒDeepSeekã€Kimi(Moonshot)ã€GPTã€Claudeç­‰ï¼ˆéƒ½å…¼å®¹OpenAIæ ¼å¼ï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¶æ„è®¾è®¡                                                  â”‚
â”‚                                                           â”‚
â”‚  DeepSeekClient  â”€â”€â”                                      â”‚
â”‚                    â”œâ”€â”€â–¶  MultiModelClient  â”€â”€â–¶  compare() â”‚
â”‚  KimiClient  â”€â”€â”€â”€â”€â”€â”˜         â”‚                            â”‚
â”‚                              â–¼                            â”‚
â”‚                      åŒä¸€é—®é¢˜å¤šæ¨¡å‹å¯¹æ¯”                      â”‚
â”‚                      (è´¨é‡ + é€Ÿåº¦ + è´¹ç”¨)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import os
import json
import time
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field
from openai import OpenAI


class DeepSeekClient:
    """
    DeepSeek API å®¢æˆ·ç«¯

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ“– æœ¯è¯­ï¼šClientï¼ˆå®¢æˆ·ç«¯ï¼‰                     â”‚
    â”‚    å°±æ˜¯"å¸®ä½ æ‰“ç”µè¯ç»™AIæœåŠ¡å™¨çš„å·¥å…·"ã€‚           â”‚
    â”‚    ä½ å‘Šè¯‰Clientè¦é—®ä»€ä¹ˆï¼ŒClientå¸®ä½ å‘è¯·æ±‚ã€     â”‚
    â”‚    æ”¶å›ç­”ã€å¤„ç†é”™è¯¯ã€‚                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä½¿ç”¨æ–¹å¼ï¼š
        from quant_core.ai import DeepSeekClient
        client = DeepSeekClient()
        answer = client.chat("åˆ†æè´µå·èŒ…å°çš„æŠ•èµ„ä»·å€¼")
    """

    # æ¨¡å‹é…ç½®
    MODELS = {
        "v3":   "deepseek-chat",        # DeepSeek-V3.2: éæ€è€ƒæ¨¡å¼
        "r1":   "deepseek-reasoner",    # DeepSeek-V3.2: æ€è€ƒæ¨¡å¼
    }

    def __init__(self, api_key: str = None, model: str = "v3"):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        å‚æ•°ï¼š
            api_key: DeepSeek API Keyã€‚
                     å¦‚æœä¸ä¼ ï¼Œä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY è¯»å–
            model:   "v3" = DeepSeek-V3ï¼ˆä¾¿å®œå¿«é€Ÿï¼‰
                     "r1" = DeepSeek-R1ï¼ˆå¼ºæ¨ç†ï¼‰
        """
        self.api_key = api_key
        if not self.api_key:
            try:
                from config import settings as _settings

                self.api_key = getattr(_settings, "DEEPSEEK_API_KEY", "") or self.api_key
            except Exception:
                pass
        if not self.api_key:
            self.api_key = os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            raise ValueError(
                "âŒ æœªæ‰¾åˆ°API Keyï¼è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š\n"
                "   export DEEPSEEK_API_KEY='ä½ çš„key'  (Mac/Linux)\n"
                "   set DEEPSEEK_API_KEY=ä½ çš„key        (Windows)\n"
                "   æˆ–è€…ç›´æ¥ä¼ å…¥ï¼šDeepSeekClient(api_key='ä½ çš„key')"
            )

        self.model_name = self.MODELS.get(model, model)

        # åˆ›å»ºOpenAIå…¼å®¹å®¢æˆ·ç«¯
        # DeepSeekçš„APIæ ¼å¼å’ŒOpenAIå®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯base_urlä¸åŒ
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com",  # DeepSeekçš„æœåŠ¡å™¨åœ°å€
        )

        self._system_prompt = None  # ç³»ç»Ÿæç¤ºè¯

    def set_system_prompt(self, prompt: str):
        """
        è®¾ç½®ç³»ç»Ÿæç¤ºè¯ï¼ˆSystem Promptï¼‰

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ğŸ“– System Prompt = AIçš„"å²—ä½è¯´æ˜ä¹¦"       â”‚
        â”‚    è®¾å®šAIçš„è§’è‰²ã€èƒ½åŠ›èŒƒå›´ã€å›ç­”è§„èŒƒã€‚       â”‚
        â”‚    æ‰€æœ‰åç»­å¯¹è¯éƒ½ä¼šå—å®ƒå½±å“ã€‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self._system_prompt = prompt
        return self  # æ”¯æŒé“¾å¼è°ƒç”¨

    def chat(
        self,
        message: str,
        temperature: float = 1.0,
        max_tokens: int = 4096,
        json_output: bool = False,
    ) -> str:
        """
        å‘é€æ¶ˆæ¯ç»™AIï¼Œè·å–å›ç­”

        å‚æ•°ï¼š
            message:     ä½ çš„é—®é¢˜/æŒ‡ä»¤
            temperature: éšæœºåº¦ï¼ˆ0=ç¡®å®š, 1=æœ‰åˆ›æ„ï¼‰é‡åŒ–åˆ†æå»ºè®®0~0.3
            max_tokens:  å›ç­”çš„æœ€å¤§é•¿åº¦ï¼ˆTokenæ•°ï¼‰
            json_output: æ˜¯å¦å¼ºåˆ¶JSONæ ¼å¼è¾“å‡º

        è¿”å›ï¼š
            AIçš„å›ç­”æ–‡æœ¬
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if self._system_prompt:
            messages.append({"role": "system", "content": self._system_prompt})
        messages.append({"role": "user", "content": message})

        # æ„å»ºè¯·æ±‚å‚æ•°
        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        # å¦‚æœè¦æ±‚JSONè¾“å‡º
        if json_output:
            kwargs["response_format"] = {"type": "json_object"}

        # è°ƒç”¨API
        response = self.client.chat.completions.create(**kwargs)

        # æå–å›ç­”æ–‡æœ¬
        return response.choices[0].message.content

    def chat_json(self, message: str, temperature: float = 0.0) -> dict:
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–JSONæ ¼å¼çš„å›ç­”ï¼ˆè‡ªåŠ¨è§£æä¸ºå­—å…¸ï¼‰

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦JSONè¾“å‡ºï¼Ÿ                       â”‚
        â”‚    é‡åŒ–ç³»ç»Ÿéœ€è¦ç¨‹åºè‡ªåŠ¨å¤„ç†AIçš„å›ç­”ã€‚           â”‚
        â”‚    è‡ªç”±æ–‡æœ¬å¾ˆéš¾è§£æï¼ŒJSONå¯ä»¥ç›´æ¥å˜æˆå­—å…¸ï¼š     â”‚
        â”‚    {"signal": "BUY", "confidence": 0.8}     â”‚
        â”‚    ç¨‹åºå°±èƒ½è¯»å– result["signal"] == "BUY"    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        text = self.chat(message, temperature=temperature, json_output=True)

        # å°è¯•è§£æJSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # æœ‰æ—¶AIè¿”å›çš„JSONå¤–é¢åŒ…äº†```json ...```ï¼Œéœ€è¦æ¸…ç†
            cleaned = text.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.split("\n", 1)[1]  # å»æ‰ç¬¬ä¸€è¡Œ
                cleaned = cleaned.rsplit("```", 1)[0]  # å»æ‰æœ€åçš„```
            return json.loads(cleaned)

    def chat_with_history(
        self,
        history: List[Dict[str, str]],
        message: str,
        temperature: float = 0.1,
        max_tokens: int = 4096,
        json_output: bool = False,
    ) -> Dict[str, object]:
        """
        å¤šè½®å¯¹è¯ï¼šæŠŠå†å²æ¶ˆæ¯ä¸€èµ·ä¼ ç»™æ¨¡å‹

        å‚æ•°ï¼š
            history:  å†å²æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º[{"role": "user|assistant|system", "content": "..."}, ...]
            message:  æœ¬è½®ç”¨æˆ·é—®é¢˜
            temperature: éšæœºåº¦
            max_tokens:  å›ç­”çš„æœ€å¤§é•¿åº¦ï¼ˆTokenæ•°ï¼‰
            json_output: æ˜¯å¦å¼ºåˆ¶JSONæ ¼å¼è¾“å‡º

        è¿”å›ï¼š
            {"reply": "...", "history": [...]}
        """
        # Build message list with history and new user prompt.
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²æ¶ˆæ¯ä¸æœ¬è½®ç”¨æˆ·é—®é¢˜ã€‚
        messages = list(history) if history else []
        messages.append({"role": "user", "content": message})

        # Reuse the same request parameters as chat().
        # å¤ç”¨ä¸chat()ä¸€è‡´çš„è¯·æ±‚å‚æ•°ã€‚
        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        if json_output:
            kwargs["response_format"] = {"type": "json_object"}

        response = self.client.chat.completions.create(**kwargs)
        reply = response.choices[0].message.content

        # Append assistant reply to history.
        # å°†AIå›å¤è¿½åŠ åˆ°å†å²ä¸­ã€‚
        messages.append({"role": "assistant", "content": reply})

        return {"reply": reply, "history": messages}

    def estimate_cost(
        self,
        input_text: str,
        output_tokens: int = 1000,
        cache_hit: bool = False,
    ) -> dict:
        """
        ä¼°ç®—æœ¬æ¬¡è°ƒç”¨çš„è´¹ç”¨

        è¿”å›ï¼š
            {"input_tokens": ..., "output_tokens": ...,
             "estimated_cost_rmb": ...}
        """
        # ç²—ç•¥ä¼°ç®—ï¼š1è‹±æ–‡å­—ç¬¦â‰ˆ0.3tokenï¼Œ1ä¸­æ–‡å­—ç¬¦â‰ˆ0.6token
        input_tokens = 0.0
        for ch in input_text:
            if ord(ch) < 128:
                input_tokens += 0.3
            else:
                input_tokens += 0.6
        input_tokens = int(input_tokens)

        # DeepSeek-V3.2 å®šä»·ï¼ˆè¾“å…¥åŒºåˆ†ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­ï¼‰
        if cache_hit:
            input_price = 0.2 / 1_000_000   # Â¥0.2/1M tokens
        else:
            input_price = 2.0 / 1_000_000   # Â¥2/1M tokens
        output_price = 3.0 / 1_000_000      # Â¥3/1M tokens

        cost = input_tokens * input_price + output_tokens * output_price

        return {
            "input_tokensï¼ˆè¾“å…¥è¯å…ƒæ•°ï¼‰": input_tokens,
            "output_tokensï¼ˆè¾“å‡ºè¯å…ƒæ•°ï¼‰": output_tokens,
            "estimated_cost_rmbï¼ˆé¢„ä¼°è´¹ç”¨/å…ƒï¼‰": round(cost, 6),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Kimi (Moonshot) å®¢æˆ·ç«¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class KimiClient:
    """
    Kimi (Moonshot) API å®¢æˆ·ç«¯

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ“– Kimi æ˜¯æœˆä¹‹æš—é¢(Moonshot AI)çš„å¤§æ¨¡å‹           â”‚
    â”‚    APIå…¼å®¹OpenAIæ ¼å¼, base_urlä¸åŒè€Œå·²ã€‚          â”‚
    â”‚    æ“…é•¿ä¸­æ–‡ç†è§£ã€é•¿ä¸Šä¸‹æ–‡(128k+)ã€è”ç½‘æœç´¢ã€‚       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä½¿ç”¨æ–¹å¼ï¼š
        from quant_core.ai import KimiClient
        client = KimiClient()
        answer = client.chat("åˆ†æè´µå·èŒ…å°çš„æŠ•èµ„ä»·å€¼")
    """

    # æ¨¡å‹é…ç½®
    MODELS = {
        "k2.5":     "kimi-k2.5",               # Kimi K2.5: æœ€æ–°å¤šæ¨¡æ€æ——èˆ°
        "k2":       "kimi-k2-0905-preview",     # Kimi K2: å¼ºAgent/ä»£ç èƒ½åŠ›
        "k2-turbo": "kimi-k2-turbo-preview",    # Kimi K2 Turbo: é«˜é€Ÿç‰ˆ
        "v1-8k":    "moonshot-v1-8k",           # Moonshot-V1: çŸ­ä¸Šä¸‹æ–‡
        "v1-32k":   "moonshot-v1-32k",          # Moonshot-V1: ä¸­ç­‰ä¸Šä¸‹æ–‡
        "v1-128k":  "moonshot-v1-128k",         # Moonshot-V1: è¶…é•¿ä¸Šä¸‹æ–‡
    }

    def __init__(self, api_key: str = None, model: str = "k2.5"):
        """
        åˆå§‹åŒ–Kimiå®¢æˆ·ç«¯

        å‚æ•°ï¼š
            api_key: Kimi API Keyã€‚
                     å¦‚æœä¸ä¼ ï¼Œä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡ KIMI_API_KEY è¯»å–
            model:   æ¨¡å‹ç®€ç§°ï¼Œå¯é€‰å€¼è§ MODELS å­—å…¸
                     "k2.5"     = Kimi K2.5ï¼ˆæ——èˆ°å¤šæ¨¡æ€ï¼‰é»˜è®¤
                     "k2"       = Kimi K2ï¼ˆå¼ºAgentï¼‰
                     "k2-turbo" = Kimi K2 Turboï¼ˆé«˜é€Ÿï¼‰
                     "v1-8k"    = Moonshot-V1 8kï¼ˆç»æµå®æƒ ï¼‰
                     "v1-32k"   = Moonshot-V1 32k
                     "v1-128k"  = Moonshot-V1 128kï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼‰
        """
        self.api_key = api_key
        if not self.api_key:
            try:
                from config import settings as _settings
                self.api_key = getattr(_settings, "KIMI_API_KEY", "") or self.api_key
            except Exception:
                pass
        if not self.api_key:
            self.api_key = os.getenv("KIMI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "âŒ æœªæ‰¾åˆ°Kimi API Keyï¼è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š\n"
                "   export KIMI_API_KEY='ä½ çš„key'  (Mac/Linux)\n"
                "   set KIMI_API_KEY=ä½ çš„key        (Windows)\n"
                "   æˆ–è€…ç›´æ¥ä¼ å…¥ï¼šKimiClient(api_key='ä½ çš„key')"
            )

        self.model_name = self.MODELS.get(model, model)

        # åˆ›å»ºOpenAIå…¼å®¹å®¢æˆ·ç«¯ï¼ˆKimi APIä¸OpenAIæ ¼å¼ä¸€è‡´ï¼‰
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.moonshot.cn/v1",  # Kimiçš„æœåŠ¡å™¨åœ°å€
        )

        self._system_prompt = None

    def set_system_prompt(self, prompt: str):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self._system_prompt = prompt
        return self

    def chat(
        self,
        message: str,
        temperature: float = 0.1,
        max_tokens: int = 4096,
        json_output: bool = False,
    ) -> str:
        """
        å‘é€æ¶ˆæ¯ç»™Kimiï¼Œè·å–å›ç­”

        å‚æ•°ä¸DeepSeekClient.chat()å®Œå…¨ä¸€è‡´ï¼Œæ–¹ä¾¿å¯¹æ¯”åˆ‡æ¢ã€‚
        """
        messages = []
        if self._system_prompt:
            messages.append({"role": "system", "content": self._system_prompt})
        messages.append({"role": "user", "content": message})

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        if json_output:
            kwargs["response_format"] = {"type": "json_object"}

        response = self.client.chat.completions.create(**kwargs)
        return response.choices[0].message.content

    def chat_json(self, message: str, temperature: float = 0.0) -> dict:
        """å‘é€æ¶ˆæ¯å¹¶è·å–JSONæ ¼å¼çš„å›ç­”ï¼ˆè‡ªåŠ¨è§£æä¸ºå­—å…¸ï¼‰"""
        text = self.chat(message, temperature=temperature, json_output=True)
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            cleaned = text.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.split("\n", 1)[1]
                cleaned = cleaned.rsplit("```", 1)[0]
            return json.loads(cleaned)

    def chat_with_history(
        self,
        history: List[Dict[str, str]],
        message: str,
        temperature: float = 0.1,
        max_tokens: int = 4096,
        json_output: bool = False,
    ) -> Dict[str, object]:
        """å¤šè½®å¯¹è¯ï¼šæŠŠå†å²æ¶ˆæ¯ä¸€èµ·ä¼ ç»™æ¨¡å‹"""
        messages = list(history) if history else []
        messages.append({"role": "user", "content": message})

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        if json_output:
            kwargs["response_format"] = {"type": "json_object"}

        response = self.client.chat.completions.create(**kwargs)
        reply = response.choices[0].message.content
        messages.append({"role": "assistant", "content": reply})

        return {"reply": reply, "history": messages}

    def estimate_cost(
        self,
        input_text: str,
        output_tokens: int = 1000,
    ) -> dict:
        """
        ä¼°ç®—Kimiè°ƒç”¨è´¹ç”¨(åŸºäºmoonshot-v1å®šä»·)

        Moonshot-V1å®šä»·: è¾“å…¥Â¥12/1M tokens, è¾“å‡ºÂ¥12/1M tokens
        """
        input_tokens = 0.0
        for ch in input_text:
            input_tokens += 0.3 if ord(ch) < 128 else 0.6
        input_tokens = int(input_tokens)

        # Moonshot-V1 ç»Ÿä¸€å®šä»·
        token_price = 12.0 / 1_000_000  # Â¥12/1M tokens
        cost = (input_tokens + output_tokens) * token_price

        return {
            "input_tokensï¼ˆè¾“å…¥è¯å…ƒæ•°ï¼‰": input_tokens,
            "output_tokensï¼ˆè¾“å‡ºè¯å…ƒæ•°ï¼‰": output_tokens,
            "estimated_cost_rmbï¼ˆé¢„ä¼°è´¹ç”¨/å…ƒï¼‰": round(cost, 6),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å¤šæ¨¡å‹å®¢æˆ·ç«¯ â€” å¯¹æ¯” & åˆ‡æ¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ModelResponse:
    """
    å•ä¸ªæ¨¡å‹çš„å›ç­”è®°å½•

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ“– ä¸ºä»€ä¹ˆè¦å•ç‹¬è®°å½•æ¯ä¸ªæ¨¡å‹çš„å›ç­”ï¼Ÿ         â”‚
    â”‚    é‡åŒ–åˆ†æéœ€è¦å¯¹æ¯”ä¸åŒAIï¼š                 â”‚
    â”‚    - è°æ›´å‡†ç¡®ï¼Ÿ(quality)                  â”‚
    â”‚    - è°æ›´å¿«ï¼Ÿ  (latency)                  â”‚
    â”‚    - è°æ›´ä¾¿å®œï¼Ÿ(cost)                     â”‚
    â”‚    ç»“æ„åŒ–è®°å½•æ‰èƒ½åšé‡åŒ–å¯¹æ¯”ã€‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    model_label: str            # æ¨¡å‹æ ‡ç­¾(å¦‚ "deepseek-v3", "kimi-v1")
    reply: str = ""             # åŸå§‹æ–‡æœ¬å›ç­”
    parsed_json: Optional[dict] = None  # è§£æåçš„JSON(å¦‚æœæ˜¯JSONè¾“å‡º)
    latency_ms: float = 0.0     # å“åº”è€—æ—¶(æ¯«ç§’)
    error: Optional[str] = None # é”™è¯¯ä¿¡æ¯(å¦‚æœå‡ºé”™)
    success: bool = True        # æ˜¯å¦æˆåŠŸ


class MultiModelClient:
    """
    å¤šæ¨¡å‹å®¢æˆ·ç«¯ â€” åŒä¸€é—®é¢˜å‘ç»™å¤šä¸ªAIï¼Œå¯¹æ¯”å›ç­”

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ“– æœ¯è¯­ï¼šMultiModelClientï¼ˆå¤šæ¨¡å‹å®¢æˆ·ç«¯ï¼‰              â”‚
    â”‚                                                       â”‚
    â”‚ å°±åƒ"åŒæ—¶é‡‡è®¿å¤šä½åˆ†æå¸ˆ"ï¼š                              â”‚
    â”‚   åŒä¸€ä¸ªå¸‚åœºé—®é¢˜ï¼Œåˆ†åˆ«é—®DeepSeekå’ŒKimi,               â”‚
    â”‚   å¯¹æ¯”ä»–ä»¬çš„è§‚ç‚¹ã€å“åº”é€Ÿåº¦ã€å›ç­”è´¨é‡,                   â”‚
    â”‚   æœ€ç»ˆå–æœ€ä¼˜æˆ–ç»¼åˆå†³ç­–ã€‚                               â”‚
    â”‚                                                       â”‚
    â”‚ é‡åŒ–åœºæ™¯ï¼š                                             â”‚
    â”‚   - A/Bæµ‹è¯•ä¸åŒæ¨¡å‹å¯¹å¸‚åœºåˆ¤æ–­çš„å‡†ç¡®ç‡                   â”‚
    â”‚   - å¤šæ¨¡å‹æŠ•ç¥¨(2ä¸ªè¯´æ¶¨ã€1ä¸ªè¯´è·Œ â†’ å€¾å‘çœ‹æ¶¨)            â”‚
    â”‚   - å¿«æ¨¡å‹åšåˆç­›ã€å¼ºæ¨¡å‹åšç²¾ç»†åˆ†æ                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä½¿ç”¨æ–¹å¼ï¼š
        from quant_core.ai import MultiModelClient
        mc = MultiModelClient()                    # é»˜è®¤åŠ è½½DeepSeek + Kimi
        results = mc.compare("åˆ†æè´µå·èŒ…å°èµ°åŠ¿")     # ä¸¤ä¸ªæ¨¡å‹åŒæ—¶å›ç­”
        mc.print_comparison(results)               # ç¾åŒ–æ‰“å°å¯¹æ¯”ç»“æœ
    """

    def __init__(self, clients: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹å®¢æˆ·ç«¯

        å‚æ•°ï¼š
            clients: æ¨¡å‹æ ‡ç­¾ â†’ å®¢æˆ·ç«¯å®ä¾‹çš„å­—å…¸
                     å¦‚æœä¸ä¼ ï¼Œè‡ªåŠ¨åˆ›å»ºDeepSeek + Kimi

        ç¤ºä¾‹ï¼š
            # æ–¹å¼1: è‡ªåŠ¨åˆ›å»º(æ¨è)
            mc = MultiModelClient()

            # æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®š
            mc = MultiModelClient({
                "deepseek-v3": DeepSeekClient(model="v3"),
                "kimi-v1":     KimiClient(model="v1-8k"),
            })

            # æ–¹å¼3: åªç”¨éƒ¨åˆ†æ¨¡å‹
            mc = MultiModelClient({
                "deepseek": DeepSeekClient(model="v3"),
            })
        """
        if clients is not None:
            self._clients = clients
        else:
            # è‡ªåŠ¨åˆ›å»º: å°è¯•åˆå§‹åŒ–æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œç¼ºkeyåˆ™è·³è¿‡
            self._clients = {}
            try:
                self._clients["deepseek-v3"] = DeepSeekClient(model="v3")
            except ValueError:
                pass
            try:
                self._clients["kimi-v1"] = KimiClient(model="v1-8k")
            except ValueError:
                pass

            if not self._clients:
                raise ValueError(
                    "âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„API Keyï¼\n"
                    "   è‡³å°‘éœ€è¦è®¾ç½® DEEPSEEK_API_KEY æˆ– KIMI_API_KEY ä¹‹ä¸€ã€‚"
                )

        self._system_prompt = None

    @property
    def model_labels(self) -> List[str]:
        """å½“å‰å·²åŠ è½½çš„æ¨¡å‹æ ‡ç­¾åˆ—è¡¨"""
        return list(self._clients.keys())

    def set_system_prompt(self, prompt: str):
        """ä¸ºæ‰€æœ‰æ¨¡å‹ç»Ÿä¸€è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self._system_prompt = prompt
        for client in self._clients.values():
            client.set_system_prompt(prompt)
        return self

    def chat(
        self,
        message: str,
        model: str = None,
        temperature: float = 1.0,
        max_tokens: int = 4096,
    ) -> str:
        """
        ç”¨æŒ‡å®šæ¨¡å‹å‘é€æ¶ˆæ¯

        å‚æ•°ï¼š
            model: æ¨¡å‹æ ‡ç­¾(å¦‚"deepseek-v3")ï¼Œä¸ä¼ åˆ™ç”¨ç¬¬ä¸€ä¸ª
        """
        label = model or self.model_labels[0]
        client = self._clients.get(label)
        if client is None:
            raise ValueError(
                f"âŒ æ¨¡å‹ '{label}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: {self.model_labels}"
            )
        return client.chat(message, temperature=temperature, max_tokens=max_tokens)

    def compare(
        self,
        message: str,
        temperature: float = 1.0,
        max_tokens: int = 4096,
        json_output: bool = False,
    ) -> List[ModelResponse]:
        """
        åŒä¸€ä¸ªé—®é¢˜å‘ç»™æ‰€æœ‰æ¨¡å‹ï¼Œæ”¶é›†å¯¹æ¯”ç»“æœ

        å‚æ•°ï¼š
            message:     é—®é¢˜/æŒ‡ä»¤
            temperature: éšæœºåº¦
            max_tokens:  æœ€å¤§è¾“å‡ºé•¿åº¦
            json_output: æ˜¯å¦è¦æ±‚JSONæ ¼å¼è¾“å‡º

        è¿”å›ï¼š
            List[ModelResponse]ï¼Œæ¯ä¸ªæ¨¡å‹ä¸€æ¡è®°å½•
        """
        results = []

        for label, client in self._clients.items():
            resp = ModelResponse(model_label=label)
            t0 = time.time()

            try:
                if json_output:
                    resp.parsed_json = client.chat_json(message, temperature=temperature)
                    resp.reply = json.dumps(resp.parsed_json, ensure_ascii=False, indent=2)
                else:
                    resp.reply = client.chat(
                        message, temperature=temperature, max_tokens=max_tokens,
                    )
            except Exception as e:
                resp.error = str(e)
                resp.success = False

            resp.latency_ms = (time.time() - t0) * 1000
            results.append(resp)

        return results

    def compare_json(
        self,
        message: str,
        temperature: float = 1.0,
    ) -> List[ModelResponse]:
        """
        åŒä¸€ä¸ªé—®é¢˜å‘ç»™æ‰€æœ‰æ¨¡å‹ï¼Œè¦æ±‚JSONæ ¼å¼å›ç­”

        è¿™æ˜¯ compare() çš„ä¾¿æ·ç‰ˆæœ¬ï¼Œé€‚åˆé‡åŒ–åˆ†æåœºæ™¯ã€‚
        """
        return self.compare(message, temperature=temperature, json_output=True)

    @staticmethod
    def print_comparison(results: List[ModelResponse]) -> None:
        """
        ç¾åŒ–æ‰“å°å¯¹æ¯”ç»“æœ

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ è¾“å‡ºæ ¼å¼:                                 â”‚
        â”‚   æ¨¡å‹å | è€—æ—¶ | çŠ¶æ€ | å›ç­”æ‘˜è¦          â”‚
        â”‚   ...å¯¹æ¯”æ€»ç»“...                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        print("\n" + "=" * 70)
        print("  ğŸ”„ å¤šæ¨¡å‹å¯¹æ¯”ç»“æœ")
        print("=" * 70)

        for i, resp in enumerate(results, 1):
            status = "âœ“ æˆåŠŸ" if resp.success else f"âœ— å¤±è´¥: {resp.error[:50]}"
            print(f"\n  â”€â”€ æ¨¡å‹{i}: {resp.model_label} â”€â”€")
            print(f"  çŠ¶æ€: {status}")
            print(f"  è€—æ—¶: {resp.latency_ms:.0f}ms")

            if resp.success:
                # æˆªå–å‰300å­—ç¬¦ä½œä¸ºæ‘˜è¦
                preview = resp.reply[:300]
                if len(resp.reply) > 300:
                    preview += "..."
                print(f"  å›ç­”æ‘˜è¦:\n    {preview}")

        # å¯¹æ¯”æ€»ç»“
        successful = [r for r in results if r.success]
        if len(successful) >= 2:
            fastest = min(successful, key=lambda r: r.latency_ms)
            slowest = max(successful, key=lambda r: r.latency_ms)
            speedup = slowest.latency_ms / fastest.latency_ms if fastest.latency_ms > 0 else 0

            print(f"\n  â”€â”€ å¯¹æ¯”æ€»ç»“ â”€â”€")
            print(f"  ğŸ† æœ€å¿«: {fastest.model_label} ({fastest.latency_ms:.0f}ms)")
            print(f"  ğŸ¢ æœ€æ…¢: {slowest.model_label} ({slowest.latency_ms:.0f}ms)")
            print(f"  âš¡ é€Ÿåº¦å·®: {speedup:.1f}x")

            # å¦‚æœæ˜¯JSONè¾“å‡ºï¼Œå¯¹æ¯”å…³é”®å­—æ®µ
            json_results = [r for r in successful if r.parsed_json]
            if json_results:
                print(f"\n  â”€â”€ JSONå­—æ®µå¯¹æ¯” â”€â”€")
                # æ‰¾å‡ºæ‰€æœ‰æ¨¡å‹å…±æœ‰çš„å­—æ®µ
                all_keys = set()
                for r in json_results:
                    all_keys.update(r.parsed_json.keys())

                # å¯¹æ¯”å…³é”®é‡åŒ–å­—æ®µ
                compare_keys = [
                    k for k in all_keys
                    if k in ("trend", "trend_cn", "strength", "confidence",
                             "risk_level", "risk_cn", "sentiment", "score",
                             "support", "resistance")
                ]
                for key in sorted(compare_keys):
                    vals = []
                    for r in json_results:
                        v = r.parsed_json.get(key, "N/A")
                        vals.append(f"{r.model_label}={v}")
                    print(f"    {key}: {' | '.join(vals)}")

        print(f"\n{'=' * 70}")

    @staticmethod
    def to_dataframe(results: List[ModelResponse]):
        """
        å°†å¯¹æ¯”ç»“æœè½¬ä¸ºDataFrame

        éœ€è¦import pandas(å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¼ºä¾èµ–)
        """
        import pandas as pd
        rows = []
        for r in results:
            row = {
                "model": r.model_label,
                "success": r.success,
                "latency_ms": round(r.latency_ms, 1),
                "reply_length": len(r.reply) if r.reply else 0,
                "error": r.error or "",
            }
            # å±•å¼€JSONå­—æ®µ
            if r.parsed_json:
                for k, v in r.parsed_json.items():
                    row[f"json_{k}"] = v
            rows.append(row)
        return pd.DataFrame(rows)